{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81d8f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download(\"punkt\")        # main tokenizer\n",
    "# nltk.download(\"punkt_tab\")    # NEW resource (needed since v3.9+)\n",
    "# nltk.download(\"averaged_perceptron_tagger\")  # for pos_tag\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76616e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f898ab62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('ner-getting-started/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2526a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from nlp-getting-started\n",
    "nlp_start_df = pd.read_csv('ner-getting-started/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2192535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_start_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695f0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take one example sentence \n",
    "ex = nlp_start_df.loc[159]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "793c6d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Experts in France begin examining airplane debris found on Reunion Island: French air accident experts on Wedn... http://t.co/v4SMAESLK5'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d749d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4579fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Experts', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('France', 'NNP'),\n",
       " ('begin', 'VB'),\n",
       " ('examining', 'VBG'),\n",
       " ('airplane', 'JJ'),\n",
       " ('debris', 'NN'),\n",
       " ('found', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('Reunion', 'NNP'),\n",
       " ('Island', 'NNP'),\n",
       " (':', ':'),\n",
       " ('French', 'JJ'),\n",
       " ('air', 'NN'),\n",
       " ('accident', 'NN'),\n",
       " ('experts', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('Wedn', 'NNP'),\n",
       " ('...', ':'),\n",
       " ('http', 'NN'),\n",
       " (':', ':'),\n",
       " ('//t.co/v4SMAESLK5', 'NN')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the sencence and apply POS tagging\n",
    "sent = pos_tag(word_tokenize(ex))\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72712f0",
   "metadata": {},
   "source": [
    "if we want to chunk only 'NN' tags, we need to use pattern\n",
    "`mychunk:{<NN>}`\n",
    "but if we need to chunk all types of tags which start with 'NN', we'll use\n",
    "`mychunk:{<NN.*>}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f78d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f63131d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Chunking (S\n",
      "  (mychunk Experts/NNS)\n",
      "  in/IN\n",
      "  (mychunk France/NNP)\n",
      "  begin/VB\n",
      "  examining/VBG\n",
      "  airplane/JJ\n",
      "  (mychunk debris/NN)\n",
      "  found/VBD\n",
      "  on/IN\n",
      "  (mychunk Reunion/NNP Island/NNP)\n",
      "  :/:\n",
      "  French/JJ\n",
      "  (mychunk air/NN accident/NN experts/NNS)\n",
      "  on/IN\n",
      "  (mychunk Wedn/NNP)\n",
      "  .../:\n",
      "  (mychunk http/NN)\n",
      "  :/:\n",
      "  (mychunk //t.co/v4SMAESLK5/NN))\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1368.0,168.0\" width=\"1368px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"5.26316%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Experts</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.63158%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"5.26316%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.43275%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"7.60234%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">France</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.2339%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09357%\" x=\"12.8655%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">begin</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.9123%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.43275%\" x=\"16.9591%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">examining</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.1754%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.84795%\" x=\"23.3918%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">airplane</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.3158%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"29.2398%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">debris</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.8713%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09357%\" x=\"34.5029%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">found</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36.5497%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"38.5965%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.7661%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.94152%\" x=\"40.9357%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Reunion</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"20px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Island</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.9064%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.75439%\" x=\"50.8772%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.7544%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.67836%\" x=\"52.6316%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">French</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.9708%\" y1=\"20px\" y2=\"48px\" /><svg width=\"14.0351%\" x=\"57.3099%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"20.8333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">air</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.4167%\" y1=\"20px\" y2=\"48px\" /><svg width=\"41.6667%\" x=\"20.8333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">accident</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"37.5%\" x=\"62.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">experts</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.25%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.3275%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.33918%\" x=\"71.345%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.5146%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"73.6842%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Wedn</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.3158%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.92398%\" x=\"78.9474%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">...</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.4094%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.26316%\" x=\"81.8713%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">http</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.5029%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.75439%\" x=\"87.1345%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.0117%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.1111%\" x=\"88.8889%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mychunk</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">//t.co/v4SMAESLK5</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.4444%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "TreeLayout(Tree('S', [Tree('mychunk', [('Experts', 'NNS')]), ('in', 'IN'), Tree('mychunk', [('France', 'NNP')]), ('begin', 'VB'), ('examining', 'VBG'), ('airplane', 'JJ'), Tree('mychunk', [('debris', 'NN')]), ('found', 'VBD'), ('on', 'IN'), Tree('mychunk', [('Reunion', 'NNP'), ('Island', 'NNP')]), (':', ':'), ('French', 'JJ'), Tree('mychunk', [('air', 'NN'), ('accident', 'NN'), ('experts', 'NNS')]), ('on', 'IN'), Tree('mychunk', [('Wedn', 'NNP')]), ('...', ':'), Tree('mychunk', [('http', 'NN')]), (':', ':'), Tree('mychunk', [('//t.co/v4SMAESLK5', 'NN')])]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "from nltk.draw.tree import TreeView\n",
    "from IPython.display import Image\n",
    "import svgling\n",
    "\n",
    "# chunk all adjacence nouns\n",
    "patterns= \"\"\"mychunk:{<NN.*>+}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "output = chunker.parse(sent)\n",
    "print(\"After Chunking\",output)\n",
    "svgling.draw_tree(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8197d4e",
   "metadata": {},
   "source": [
    "Similarly as part-of-speech tags, IOB tags are a slightly different way for representing chunk structures. This format can denote the inside, outside, and beginning of a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e068ccf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Experts', 'NNS', 'B-mychunk'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('France', 'NNP', 'B-mychunk'),\n",
       " ('begin', 'VB', 'O'),\n",
       " ('examining', 'VBG', 'O'),\n",
       " ('airplane', 'JJ', 'O'),\n",
       " ('debris', 'NN', 'B-mychunk'),\n",
       " ('found', 'VBD', 'O'),\n",
       " ('on', 'IN', 'O'),\n",
       " ('Reunion', 'NNP', 'B-mychunk'),\n",
       " ('Island', 'NNP', 'I-mychunk'),\n",
       " (':', ':', 'O'),\n",
       " ('French', 'JJ', 'O'),\n",
       " ('air', 'NN', 'B-mychunk'),\n",
       " ('accident', 'NN', 'I-mychunk'),\n",
       " ('experts', 'NNS', 'I-mychunk'),\n",
       " ('on', 'IN', 'O'),\n",
       " ('Wedn', 'NNP', 'B-mychunk'),\n",
       " ('...', ':', 'O'),\n",
       " ('http', 'NN', 'B-mychunk'),\n",
       " (':', ':', 'O'),\n",
       " ('//t.co/v4SMAESLK5', 'NN', 'B-mychunk')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "\n",
    "iob_tagged = tree2conlltags(output)\n",
    "iob_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7257218",
   "metadata": {},
   "source": [
    "Recognizing a named entity is a specific kind of chunk extraction that uses entity tags along with chunk tags. Common entity tags include PERSON, LOCATION, and ORGANIZATION. NLTK has already a pre-trained named entity chunker which can be used using ne_chunk() method in the nltk.chunk module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f4e4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker_tab')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dacd22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "ne_res = ne_chunk(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d301b0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1304.0,168.0\" width=\"1304px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"5.52147%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Experts</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.76074%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.45399%\" x=\"5.52147%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.74847%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.90798%\" x=\"7.97546%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">France</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.4294%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.29448%\" x=\"12.8834%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">begin</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.0307%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.74847%\" x=\"17.1779%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">examining</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.5521%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.13497%\" x=\"23.9264%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">airplane</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.9939%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.90798%\" x=\"30.0613%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">debris</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"32.5153%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.29448%\" x=\"34.9693%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">found</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.1166%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.45399%\" x=\"39.2638%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.4908%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4294%\" x=\"41.7178%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Reunion</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"20px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Island</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.9325%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.84049%\" x=\"52.1472%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0675%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.90798%\" x=\"53.9877%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">French</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.4417%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.06748%\" x=\"58.8957%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">air</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.4294%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.13497%\" x=\"61.9632%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">accident</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.0307%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.52147%\" x=\"68.0982%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">experts</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.8589%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.45399%\" x=\"73.6196%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.8466%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.68098%\" x=\"76.0736%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Wedn</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.9141%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.06748%\" x=\"79.7546%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">...</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.2883%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.68098%\" x=\"82.8221%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">http</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.6626%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.84049%\" x=\"86.5031%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.4233%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.6564%\" x=\"88.3436%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">//t.co/v4SMAESLK5</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.1718%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('Experts', 'NNS'), ('in', 'IN'), Tree('GPE', [('France', 'NNP')]), ('begin', 'VB'), ('examining', 'VBG'), ('airplane', 'JJ'), ('debris', 'NN'), ('found', 'VBD'), ('on', 'IN'), Tree('ORGANIZATION', [('Reunion', 'NNP'), ('Island', 'NNP')]), (':', ':'), Tree('GPE', [('French', 'JJ')]), ('air', 'NN'), ('accident', 'NN'), ('experts', 'NNS'), ('on', 'IN'), Tree('GPE', [('Wedn', 'NNP')]), ('...', ':'), ('http', 'NN'), (':', ':'), ('//t.co/v4SMAESLK5', 'NN')])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e86b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ne(trees, labels):\n",
    "    \n",
    "    ne_list = []\n",
    "    for tree in ne_res:\n",
    "        if hasattr(tree, 'label'):\n",
    "            if tree.label() in labels:\n",
    "                ne_list.append(tree)\n",
    "    \n",
    "    return ne_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06fd8fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ORGANIZATION', [('Reunion', 'NNP'), ('Island', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "labels = ['ORGANIZATION']\n",
    "print(extract_ne(ne_res, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1a08f",
   "metadata": {},
   "source": [
    "**spaCy** features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default trained pipelines can identify a variety of named and numeric entities, including companies, locations, organizations and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df84dc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikibooks: Radiation Oncology/NHL/CLL-SLL</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Radiation_Oncolo...</td>\n",
       "      <td>Chronic Lymphocytic Leukemia and Small Lymphoc...</td>\n",
       "      <td>Front Page: Radiation Oncology | RTOG Trials |...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;table width=\"10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikibooks: Romanian/Lesson 9</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Romanian/Lesson_9</td>\n",
       "      <td>==Băuturi/Beverages==</td>\n",
       "      <td>Băuturi/Beverages[edit | edit source]\\nTea : C...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h2&gt;&lt;span id=\"B....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikibooks: Karrigell</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Karrigell</td>\n",
       "      <td>Karrigell is an open Source Python web framewo...</td>\n",
       "      <td>Karrigell is an open Source Python web framewo...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;p&gt;Karrigell is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikibooks: The Pyrogenesis Engine/0 A.D./GuiSe...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/The_Pyrogenesis_...</td>\n",
       "      <td>====setupUnitPanel====</td>\n",
       "      <td>setupUnitPanel[edit | edit source]\\nHelper fun...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;h4&gt;&lt;span class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wikibooks: LMIs in Control/pages/Exterior Coni...</td>\n",
       "      <td>https://en.wikibooks.org/wiki/LMIs_in_Control/...</td>\n",
       "      <td>== The Concept ==</td>\n",
       "      <td>Contents\\n\\n1 The Concept\\n2 The System\\n3 The...</td>\n",
       "      <td>&lt;div class=\"mw-parser-output\"&gt;&lt;div id=\"toc\" cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0          Wikibooks: Radiation Oncology/NHL/CLL-SLL   \n",
       "1                       Wikibooks: Romanian/Lesson 9   \n",
       "2                               Wikibooks: Karrigell   \n",
       "3  Wikibooks: The Pyrogenesis Engine/0 A.D./GuiSe...   \n",
       "4  Wikibooks: LMIs in Control/pages/Exterior Coni...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://en.wikibooks.org/wiki/Radiation_Oncolo...   \n",
       "1    https://en.wikibooks.org/wiki/Romanian/Lesson_9   \n",
       "2            https://en.wikibooks.org/wiki/Karrigell   \n",
       "3  https://en.wikibooks.org/wiki/The_Pyrogenesis_...   \n",
       "4  https://en.wikibooks.org/wiki/LMIs_in_Control/...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Chronic Lymphocytic Leukemia and Small Lymphoc...   \n",
       "1                              ==Băuturi/Beverages==   \n",
       "2  Karrigell is an open Source Python web framewo...   \n",
       "3                             ====setupUnitPanel====   \n",
       "4                                  == The Concept ==   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  Front Page: Radiation Oncology | RTOG Trials |...   \n",
       "1  Băuturi/Beverages[edit | edit source]\\nTea : C...   \n",
       "2  Karrigell is an open Source Python web framewo...   \n",
       "3  setupUnitPanel[edit | edit source]\\nHelper fun...   \n",
       "4  Contents\\n\\n1 The Concept\\n2 The System\\n3 The...   \n",
       "\n",
       "                                           body_html  \n",
       "0  <div class=\"mw-parser-output\"><table width=\"10...  \n",
       "1  <div class=\"mw-parser-output\"><h2><span id=\"B....  \n",
       "2  <div class=\"mw-parser-output\"><p>Karrigell is ...  \n",
       "3  <div class=\"mw-parser-output\"><h4><span class=...  \n",
       "4  <div class=\"mw-parser-output\"><div id=\"toc\" cl...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "cnx = sqlite3.connect('ner-getting-started/wikibooks.sqlite')\n",
    "df_wikibooks = pd.read_sql_query(\"SELECT * FROM en\", cnx)\n",
    "df_wikibooks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb8673",
   "metadata": {},
   "source": [
    "spaCy’s trained pipelines can be installed as Python packages. This means that they’re a component of your application, just like any other module. They’re versioned and can be defined as a dependency in your requirements.txt. Trained pipelines can be installed from a download URL or a local directory, manually or via pip. Their data can be located anywhere on your file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1af767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "535c5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91c71e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This Wikibooks page is a fact sheet and analysis on the article \"Habitual physical activity in children and adolescents with cystic fibrosis\" about how exercise is related to the disease Cystic Fibrosis.\n",
       "\n",
       "Contents\n",
       "\n",
       "1 Background of this research\n",
       "2 Where is the research from ?\n",
       "3 What kind of research was this?\n",
       "4 What did the research involve?\n",
       "\n",
       "4.1 Pulmonary Function testing\n",
       "4.2 Pros / Cons of this test\n",
       "\n",
       "\n",
       "5 What were the basic results?\n",
       "6 What conclusion can we take from this research ?\n",
       "7 Practical Advice\n",
       "8 Further information/ Resources\n",
       "\n",
       "8.1 Cystic Fibrosis Australia\n",
       "8.2 Cystic Fibrosis's National Ambassador Nathan Charles\n",
       "\n",
       "\n",
       "9 References\n",
       "\n",
       "\n",
       "\n",
       "Background of this research[edit | edit source]\n",
       "The research was about the effects of taking part in exercise constantly or making it a habit in the population of children and teens that are severing from the genetic condition cystic Fibrosis.\n",
       "What is  Cystic Fibrosis\n",
       "It is a genetic condition, affecting lungs and digestion. Unfortunately, there is no cure. The condition Cystic Fibrosis (CF) is mostly inherited in the white population with 1 in every 3300 live births being diagnosed with the condition.[1]\n",
       "\n",
       "Where is the research from ?[edit | edit source]\n",
       "This research was based in the American Children’s hospital Pittsburgh in the CF centre. Volunteers for this research included siblings, friends and hospital employee’s children who did not have the condition. Two authors of this research work within the department as paediatrics and others have conducted research regarding children with CF. This included David Michael Orenstein who has many publications on CF. These authors have also conducted other research on children with CF with methods of exercise that can help combat the condition.[2]\n",
       "\n",
       "What kind of research was this?[edit | edit source]\n",
       "This was a meta-analysis form of research; even though this kind of research is time consuming the results are valid and reliable. Other studies that have been done have very similar results, regarding the effects of physical activity and the benefits it has on children and adolescents with CF. For an example, a study that was conducted in Austria compared the effects of physical activity versus chest physiotherapy which is popular within the CF community.[3] Two of the authors, David Michael and Patricia from the research article have conducted a study of “The prognostic value of exercise testing in patients with CF”.[2] Also, the Journal of Paediatric Pulmonology had similar conclusions that through exercise there is an improvement in oxygen consumption and physical self-efficiency and appearance in patients. As well as, lots of positive changes in living conditions of the patients.[4] Even though the research method used in these three studies differ, they all have very similar conclusions that exercise is beneficial for children with CF.\n",
       "\n",
       "What did the research involve?[edit | edit source]\n",
       "60 people in total 7–17 years of age[5]\n",
       "30 Patients with cystic fibrosis  (18 male, 12 female)[5]\n",
       "30 people in the control not affected (17 male, 13 female )[5]\n",
       "The participants completed a Questionnaire about their activity levels. Children 12 years and older completed it with no help or little assistant. Children 12 years and under did it with a parent or guardian. \n",
       "When getting tested the children did 2 types of tests, a Pulmonary Function test, and an Exercise Test. The level of aerobic fitness was tested by the participant completing a progressive exercise test on a stationary electronic bike (cycle ergometer) using the Godfrey protocol. Oxygen uptake was measured using a cart that you breathed into and then It  analysed the breath content. This was recorded during the last 15 seconds of each stage exercise\n",
       "\n",
       "Pulmonary Function testing[edit | edit source]\n",
       "Pulmonary function was tested before exercising. Children who have CF had limited experience in doing these tests as they did not have regular exposure to the test due to the condition. A spirometry was used to measure pulmonary lung function capacity. The aim of the test is to measure how much and how quickly an individual is able to move air out of their lungs” ([6] this is done by breathing into a mouth piece connected to a device that records the air and it called a spirometer.\n",
       "\n",
       "Pros / Cons of this test[edit | edit source]\n",
       "This study was very good for testing but there were disadvantages on the younger population in the study due to being short as they were  unable to reach the pedals. Another limitation of the study is that focus was only on the effects of aerobic training and did not take into account the benefit of anaerobic or resistance training can have on an individual. Also, the Australian Cystic Fibrosis Council suggest that core strength is also an important component of helping with the clearance of mucus for patients[1]\n",
       "\n",
       "What were the basic results?[edit | edit source]\n",
       "The survival rate of living with Cystic Fibrosis is affected by the engagement of regulary physical Activity\n",
       "The oxygen consumption improves with exercise.\n",
       "Exercise helps with the removal of mucus\n",
       "Children with Cystic Fibrosis participate in less vigorous physical exercise and activities when compared with children not affected by CF\n",
       "What conclusion can we take from this research ?[edit | edit source]\n",
       "In conclusion, this research demonstrates that exercise does have benefit's for children living with CF as it increases the survival rate and increase life expectancy. I believe one thing that is important when trying to help treat children with CF is to treat them normally and allowing them to engage in the activity as their peers are doing, within reason.\n",
       "\n",
       "Practical Advice[edit | edit source]\n",
       "Before trying to treat CF with exercise consult Doctors about the type of exercise and don’t push yourself too hard. Build up the intensity.\n",
       "\n",
       "Further information/ Resources[edit | edit source]\n",
       "Cystic Fibrosis Australia[edit | edit source]\n",
       "Cystic Fibrosis Australia even suggests that exercise is an important component of treating cystic fibrosis as it help with clearing the airways and building core strength.[1]\n",
       "Web Page: http://www.cysticfibrosis.org.au\n",
       "\n",
       "Cystic Fibrosis's National Ambassador Nathan Charles[edit | edit source]\n",
       "Cystic Fibrosis's National Ambassador Nathan Charles an elite rugby union player playing a contact sport while living with the condition cystic fibrosis. Shows that it is possible to stay fit and achieve great success with cystic fibrosis.[7]\n",
       "Nathan Charles Web page http://nathancharles.com.au\n",
       "Playing Elite Rugby with CF:  http://nathancharles.com.au/nutri-grain-unstoppable/\n",
       "\n",
       "References[edit | edit source]\n",
       "\n",
       "\n",
       "↑ a b c Cystic Fibrosis [Internet]. Cysticfibrosis.org.au. 2016 [cited 24 September 2016]. Available from: http://www.cysticfibrosis.org.au/all/learn/\n",
       "\n",
       "↑ a b Nixon P, Orenstein D, Kelsey S, Doershuk C. The prognostic value of exercise testing in patients with cystic fibrosis [Internet]. Saskatoon Public Library. 2010 [cited 15 September 2016]. Available from: http://saskatoonlibrary.ca/eds/item?dbid=edsgea&an=edsgcl.13305971\n",
       "\n",
       "↑ M. Orenstein D, A. Nixon P, A. Washburn , F. Kelsey S. Measuring Physical Activity in Children with Cystic Fibrosis: Comparison of Four Methods: Paediatric Exercise Science: Vol 5, No 2. Paediatric Exercise Science [Internet]. 2016 [cited 13 September 2016];5(2):125-133. Available from: http://journals.humankinetics.com/doi/pdf/10.1123/pes.5.2.125\n",
       "\n",
       "↑ Gulmans V, de Meer K, Brackel H, Faber J, Berger R, Helders P. Outpatient exercise training in children with cystic fibrosis: Physiological effects, perceived competence, and acceptability. Pediatric Pulmonology [Internet]. 1999 [cited 15 September 2016];28(1):39-46. Available from: http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-0496(199907)28:1%3C39::AID-PPUL7%3E3.0.CO;2-8/abstract\n",
       "\n",
       "↑ a b c NIXON P, ORENSTEIN D, KELSEY S. Habitual physical activity in children and adolescents with CF. Medicine and Science in Sports and Exercise [Internet]. 2001 [cited 2 September 2016];33(1):30-35. Available from: http://zh9bf5sp6t.scholar.serialssolutions.com/?sid=google&auinit=PA&aulast=Nixon&atitle=Habitual+physical+activity+in+children+and+adolescents+with+cystic+fibrosis.&id=pmid:1119410\n",
       "\n",
       "↑ Lung Function Tests [Internet]. WebMD. 2016 [cited 14 September 2016]. Available from: http://www.webmd.com/lung/lung-function-tests\n",
       "\n",
       "↑ Charles N. NATIONAL AMBASSADOR FOR CYSTIC FIBROSIS AUSTRALIA [Internet]. Nathan Charles. 2015 [cited 25 September 2016]. Available from: http://nathancharles.com.au/bio/"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_ex = df_wikibooks.iloc[11]['body_text']\n",
    "doc = nlp(wiki_ex)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70e81c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40d63853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entity types that spacy recognised from the document above\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CARDINAL', 'DATE', 'EVENT', 'GPE', 'LOC', 'ORG', 'PERSON', 'PRODUCT', 'TIME'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('All entity types that spacy recognised from the document above')\n",
    "set([ent.label_ for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0dbf474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persons from the document above\n",
      "{Nixon, KELSEY S. Habitual, Nathan Charles, A. Nixon P, Nathan Charles, ↑ Charles N. NATIONAL, David Michael Orenstein, Patricia, F. Kelsey S. Measuring Physical Activity, Berger, David Michael, A. Washburn, Nathan Charles, Nathan, Nathan Charles Web}\n",
      "Organizations from the document above\n",
      "{Pros / Cons, Godfrey, Cystic Fibrosis, Cystic Fibrosis, Practical Advice, CF, the Journal of Paediatric Pulmonology, National, CF, CF, CF, Kelsey S, the Australian Cystic Fibrosis Council, the American Children’s, Pulmonary Function, Cystic Fibrosis, Cystic Fibrosis, CF, Brackel H, CF, Cystic Fibrosis, Cystic Fibrosis's, CF, Children with Cystic Fibrosis, National, the CF community.[3] Two, Pediatric Pulmonology, CF, CF}\n"
     ]
    }
   ],
   "source": [
    "print('Persons from the document above')\n",
    "print(set([ent for ent in doc.ents if ent.label_ == 'PERSON']))\n",
    "print('Organizations from the document above')\n",
    "print(set([ent for ent in doc.ents if ent.label_ == 'ORG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "103a9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "# displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0a739",
   "metadata": {},
   "source": [
    "BERT NER\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a neural network that is capable of parsing language in the same way a human does. It uses word embeddings to translate words into numbers and then back again, allowing it to understand word context and meaning.\n",
    "Hugging face is one of the most used NLP libraries. With this library, we can leverage popular NLP models, such as BERT, DistilBERT roBERTa and use those models to manipulate text in one way or another. Hugging Face provides simple access to a variety of models and datasets used for all possible NLP tasks.\n",
    "Models for NER tasks can be found under the \"Token Classification\" section here:\n",
    "https://huggingface.co/models?pipeline_tag=token-classification&sort=trending\n",
    "\n",
    "Great Hugging Face course about transformers and the usage of the library can be found here https://huggingface.co/course/chapter0?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74734c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d75fa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eccd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d53646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nlp_start_df = pd.read_csv('ner-getting-started/train.csv')\n",
    "# take one example sentence \n",
    "ex = nlp_start_df.loc[159]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94f9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Experts in France begin examining airplane debris found on Reunion Island: French air accident experts on Wedn... http://t.co/v4SMAESLK5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590f4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # optional, just to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e179854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "c:\\Users\\ishar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9996474),\n",
       "  'word': 'France',\n",
       "  'start': 11,\n",
       "  'end': 17},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9992249),\n",
       "  'word': 'Re',\n",
       "  'start': 59,\n",
       "  'end': 61},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.8416335),\n",
       "  'word': '##union Island',\n",
       "  'start': 61,\n",
       "  'end': 73},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': np.float32(0.9997576),\n",
       "  'word': 'French',\n",
       "  'start': 75,\n",
       "  'end': 81}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"ner\",\n",
    "                     model=\"dslim/bert-base-NER\",\n",
    "                     grouped_entities=True)\n",
    "generator(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a11dc",
   "metadata": {},
   "source": [
    "The process of training a neural network is a difficult and time-consuming process and for most of the users not even feasible. Because of that, instead of training the model from scratch, we can use models from Hugging Face which has been trained using a large amount of text.\n",
    "\n",
    "These types of models through training developed a statistical understanding of the language they have been trained on, but they might not be useful for our specific task. In order to utilize the knowledge of the model, we can apply fine-tuning. It means that we can take pretrained model and train it a little bit more with our annotated data.\n",
    "\n",
    "This process is called transfer learning when the knowledge is transfered from one model to another one and that strategy is often used in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150dade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence: 1</th>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10</th>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 100</th>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 1000</th>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10000</th>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Word  \\\n",
       "Sentence #                                                           \n",
       "Sentence: 1      [Thousands, of, demonstrators, have, marched, ...   \n",
       "Sentence: 10     [Iranian, officials, say, they, expect, to, ge...   \n",
       "Sentence: 100    [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "Sentence: 1000   [They, left, after, a, tense, hour-long, stand...   \n",
       "Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                               POS  \\\n",
       "Sentence #                                                           \n",
       "Sentence: 1      [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "Sentence: 10     [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "Sentence: 100    [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "Sentence: 1000      [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "Sentence: 10000  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "\n",
       "                                                               Tag  \n",
       "Sentence #                                                          \n",
       "Sentence: 1      [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "Sentence: 10     [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "Sentence: 100    [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...  \n",
       "Sentence: 1000                   [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "Sentence: 10000  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('ner_dataset.csv', encoding='ISO-8859-1')\n",
    "df['Sentence #'] = df['Sentence #'].ffill()\n",
    "df_gr = df.groupby('Sentence #').agg(lambda x: list(x))\n",
    "\n",
    "df_gr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a233b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #           Word  POS Tag\n",
       "0            Sentence: 1      Thousands  NNS   O\n",
       "1            Sentence: 1             of   IN   O\n",
       "2            Sentence: 1  demonstrators  NNS   O\n",
       "3            Sentence: 1           have  VBP   O\n",
       "4            Sentence: 1        marched  VBN   O\n",
       "...                  ...            ...  ...  ..\n",
       "1048570  Sentence: 47959           they  PRP   O\n",
       "1048571  Sentence: 47959      responded  VBD   O\n",
       "1048572  Sentence: 47959             to   TO   O\n",
       "1048573  Sentence: 47959            the   DT   O\n",
       "1048574  Sentence: 47959         attack   NN   O\n",
       "\n",
       "[1048575 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ea8b0",
   "metadata": {},
   "source": [
    "In our case, we'll need columns Word(tokenized sentence) and Tag(entities in the sentence). Also, in order to fine-tune the model, we'll need to have the same entities as the model is trained on. Let's print entities from our data set and from the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3b2370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in our data set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = []\n",
    "for tag in df_gr['Tag'].to_list():\n",
    "    tags.extend(tag)\n",
    "print('Entities in our data set')\n",
    "set(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eac11cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities from the pretrained model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "print('Entities from the pretrained model')\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "569b8d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities lables from the pretrained model\n",
      "O\n",
      "B-MISC\n",
      "I-MISC\n",
      "B-PER\n",
      "I-PER\n",
      "B-ORG\n",
      "I-ORG\n",
      "B-LOC\n",
      "I-LOC\n"
     ]
    }
   ],
   "source": [
    "print('Entities lables from the pretrained model')\n",
    "for label in model.config.id2label.values():\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adbfc1",
   "metadata": {},
   "source": [
    "Create the mapping for converting entities from our data set into the entity id (key) from the 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4ff251",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mapping = {\n",
    "'O':0,'B-per':3, 'I-per':4, 'B-org':5, 'I-org':6,'B-geo':7, 'I-geo':8,\n",
    "'B-art':1, 'B-eve':1 , 'B-gpe':1, 'B-nat':1, 'B-tim':1,\n",
    "'I-art':1, 'I-eve':1 , 'I-gpe':1, 'I-nat':1, 'I-tim':1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e55cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset:\n",
    "    def __init__(self, df):\n",
    "        # input is annotated data frame\n",
    "        self.texts = df['Word'].to_list()\n",
    "        self.tags = df['Tag'].to_list()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        tags = self.tags[item]\n",
    "        \n",
    "        ids = []\n",
    "        target_tag =[]\n",
    "        \n",
    "        # tokenize words and define tags accordingly\n",
    "        # running -> [run, ##ning]\n",
    "        # tags - ['O', 'O']\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = tokenizer.encode(s, add_special_tokens=False)\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            target_tag.extend([entity_mapping[tags[i]]] * input_len)\n",
    "        \n",
    "        # truncate\n",
    "        ids = ids[:MAX_LEN - 2]\n",
    "        target_tag = target_tag[:MAX_LEN - 2]\n",
    "        \n",
    "        # add special tokens\n",
    "        ids = [101] + ids + [102]\n",
    "        target_tag = [0] + target_tag + [0]\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "        \n",
    "        # construct padding\n",
    "        padding_len = MAX_LEN - len(ids)\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "        \n",
    "        return {'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'labels': torch.tensor(target_tag, dtype=torch.long)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5e7492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Split the data into training and validation sets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df_train, df_val = train_test_split(\u001b[43mdf_gr\u001b[49m, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Split the validation set into a new test set\u001b[39;00m\n\u001b[32m      9\u001b[39m df_val, df_test = train_test_split(df_val, test_size=\u001b[32m0.5\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_gr' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "df_train, df_val = train_test_split(df_gr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the validation set into a new test set\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "MAX_LEN = 256 ##128\n",
    "\n",
    "data_train = NERDataset(df_train)\n",
    "data_val = NERDataset(df_val)\n",
    "data_test = NERDataset(df_test)\n",
    "\n",
    "# initialize DataLoader used to return batches for training/validation\n",
    "loader_train = torch.utils.data.DataLoader(\n",
    "    data_train, batch_size=32, num_workers=4\n",
    ")\n",
    "\n",
    "loader_val = torch.utils.data.DataLoader(\n",
    "    data_val, batch_size=32, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68579006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NERDataset at 0x235add99450>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "570450d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x235add99950>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda50531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForTokenClassification, get_scheduler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038d9c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# just train the linear classifier on top of BERT\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m param_optimizer = \u001b[38;5;28mlist\u001b[39m(\u001b[43mmodel\u001b[49m.classifier.named_parameters())\n\u001b[32m      3\u001b[39m optimizer_grouped_parameters = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: [p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m param_optimizer]}]\n\u001b[32m      4\u001b[39m optimizer = AdamW(\n\u001b[32m      5\u001b[39m     optimizer_grouped_parameters,\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# lr=3e-5,\u001b[39;00m\n\u001b[32m      7\u001b[39m     lr = \u001b[32m5e-5\u001b[39m,\n\u001b[32m      8\u001b[39m     eps=\u001b[32m1e-12\u001b[39m\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# just train the linear classifier on top of BERT\n",
    "param_optimizer = list(model.classifier.named_parameters())\n",
    "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    # lr=3e-5,\n",
    "    lr = 5e-5,\n",
    "    eps=1e-12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5138bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77d02f0914f424fbe4f847ceafcb0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## full finetuning\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# add scheduler to linearly reduce the learning rate throughout the epochs.\n",
    "num_epochs = 1 ##3\n",
    "num_training_steps = num_epochs * len(loader_train)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps),refresh=True)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in loader_train:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        true_labels.extend(batch['labels'].detach().cpu().numpy().ravel())\n",
    "        predictions.extend(np.argmax(outputs[1].detach().cpu().numpy(), axis=2).ravel())\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        final_loss+=loss.item()\n",
    "        \n",
    "    print(f'Training loss: {final_loss/len(loader_train)}')\n",
    "    print('Training F1: {}'.format(f1_score(predictions, true_labels, average='macro')))\n",
    "    print(f'Training acc: {accuracy_score(predictions, true_labels)}')\n",
    "    print('*'*20)\n",
    "    \n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in loader_val:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        final_loss+=outputs.loss.item()\n",
    "        true_labels.extend(batch['labels'].detach().cpu().numpy().ravel())\n",
    "        predictions.extend(np.argmax(outputs[1].detach().cpu().numpy(), axis=2).ravel())\n",
    "    print(f'Validation loss: {final_loss/len(loader_val)}')\n",
    "    print('Vallidation F1: {}'.format(f1_score(predictions, true_labels, average='macro')))\n",
    "    print(f'Validaton acc: {accuracy_score(predictions, true_labels)}')\n",
    "    print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# test the model\n",
    "test_sentence = \"\"\"\n",
    "Mr. Trump’s tweets began just moments after a Fox News report by Mike Tobin, a \n",
    "reporter for the network, about protests in Minnesota and elsewhere. \n",
    "\"\"\"\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "# join bpe split tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(label_idx)\n",
    "        new_tokens.append(token)\n",
    "        \n",
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t{}\".format(model.config.id2label[label], token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Save the trained model in the same directory as the notebook\n",
    "model_path = os.path.join(current_directory, \"model.pth\")\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
